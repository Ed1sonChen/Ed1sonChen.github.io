<!DOCTYPE html>

<html lang="en">
    <head>

        <!-- Metadata -->
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
        <meta name="description" content="website"/>
        <meta name="author" content="Zhiling Chen"/>
        <title>Zhiling Chen @ UCONN</title>
        <link rel="icon" type="image/x-icon" href="assets\img\batman_icon.png"/>
        
        <!-- Font Awesome icons -->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js"></script>
		
        <!-- Google fonts-->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">        
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css"/>
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.theme.default.min.css">

        <!-- Core theme CSS -->
        <link href="styles/styles.css" rel="stylesheet"/>

    </head>

    <body class="light-theme">

        <!-- Moving particles -->
        <canvas id="canvas"></canvas>

        <!-- Progress bar on top -->
        <div class="progress-bar-container">
            <div class="progress-bar" id="progressBar"></div>
        </div>

        <!-- Back to top button -->
        <a id="back-to-top-button"></a>

        <!-- Toggle dark/light theme button -->
        <button class="toggle-theme-button" onclick="toggleTheme()">‚òÄÔ∏è</button>
        
        <!-- Assitant icon saying about theme changes -->
        <div class="popup-icon-container" id="popupIconContainer" draggable="true">
            <div class="icon"><img src="assets\img\batman_icon.png" width="65" height="65"></div>
            <div class="speech-balloon"></div>
        </div>
        
        <!-- Dismissal area for assistant icon -->
        <div class="dismissal-area" id="dismissalArea">&#10006;</div>

        <!-- Content -->
        <div class="container mt-5">
            <!-- About section -->

<div class="row mb-4">
    <div class="col-lg-2 col-md-4">
        <div class="ring-container">
            <div class="ring">
                <div class="hollow-ring">
                    <img class="profile-image" src="assets\img\me.jpg" alt="Zhiling Chen" />
                     
                        <div class="emoji-indicator">
                            üçÄ 
                            <span class="hover-text"> four-leaf clover </span> 
                        </div> 
                    
                </div>
            </div>
        </div>
        <hr />
        <div class="social-icons">	
             <!-- <a class="social-icon" href="https://scholar.google.com/citations?user=Z6_5ZTEAAAAJ" target="_blank" rel="noopener" title="Google Scholar"><i class="fa fa-graduation-cap" style="font-size: 35px; color: #4285f4"></i></a>  -->
             <!-- <a class="social-icon" href="https://www.researchgate.net/profile/Khang-Nguyen-133" target="_blank" rel="noopener" title="ResearchGate"><i class="fab fa-researchgate" style="font-size: 35px; color: #00D0BB"></i></a>  -->
             <a class="social-icon" href="https://github.com/Ed1sonChen" target="_blank" rel="noopener" title="GitHub"><i class="fab fa-github" style="font-size: 35px; color: #171515"></i></a> 
             <!-- <a class="social-icon" href="https://www.youtube.com/@_m.khangg_" target="_blank" rel="noopener" title="YouTube"><i class="fab fa-youtube" style="font-size: 35px; color: #FF0000"></i></a>  -->
             <a class="social-icon" href="assets\doc\CV.pdf" target="_blank" rel="noopener" title="Resume"><i class="fas fa-file-alt" style="font-size: 35px; color: #bd5d38"></i></a> 
        </div>
        <p></p>
    </div>

    <div class="col-lg-10 col-md-8">
        <h2>Zhiling Chen</h2>
        <p></p>
        I am currently a first-year Mechanical Engineering PhD student at the <a href="https://me.engr.uconn.edu/" target="_blank" rel="noopener">University of Connecticut</a> and a research assistant at the <hightlight><a href="https://imani.lab.uconn.edu/" target="_blank" rel="noopener">Intelligent Systems and Control Laboratory (ISCL)</a></hightlight>, where I am advised by <a href="https://scholar.google.com/citations?user=XIdSChwAAAAJ&hl=en" target="_blank" rel="noopener">Dr. Farhad Imani</a>. (‚óï‚ñø‚óï)
        <p></p>
        I grew up in <a href="https://en.wikipedia.org/wiki/Wuxi" target="_blank" rel="noopener">Wuxi, China</a>      
        <p></p>
        <code>&gt;&gt; I do research in Distributed Learning, Task specified AI and Spatial Intelligence.  </code>
        <!-- <p></p> -->
    </div>
</div>

<!-- Updates section -->

<hr />

<div class="row" id="updates">
    <div class="col">
        <h2 clss="mb-5">üìç updates</h2>
        <p></p>
        <div class="owl-carousel owl-theme">
            
                <div class="news-card">
    <img src="assets/img/updates_gan/city_boy.jpg" class="w-full rounded-lg" />
    <div class="news-desc">waiting for update <hightlight>storrs, CT</hightlight>.</div>
    <div class="news-time">October 2024</div>
</div>
            
                <!-- <div class="news-card">
    <img src="assets/img/updates_gan/travel_boy.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I will present at ISR 2023 in <hightlight>Stuttgart, Germany</hightlight>.</div>
    <div class="news-time">September 2023</div>
</div>
            
            
                <div class="news-card">
    <img src="assets/img/updates_gan/boy_school.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I start my undergraduate at the <hightlight>University of Texas at Arlington (UTA)</hightlight>.</div>
    <div class="news-time">August 2020</div>
</div> -->
            
        </div>
        <p></p>
    </div>
</div>

<!-- Research section -->

<hr />

<div class="row" id="research">
    <div class="col">
        <h2 clss="mb-5">üìç publications</h2>
        <p></p>
        <div id="filters">
            <button class="filter-button active" data-filter="*">all</button>
            
                <button class="filter-button" data-filter="perception manipulation">perception + manipulation</button>
            
                <button class="filter-button" data-filter="framework">framework</button>
            
        </div>
        <p></p>
        <div id="projects" class="isotope">
            
                <div class="project" data-filter="perception manipulation">
    <div class="row mb-4">
        <div class="col-sm-4">
            <!-- <img width="100%" height="auto" class="w-full rounded-lg" src="C:\Users\zhc23013\Desktop\mywebsite\assets\img\Distributed.png" /> -->
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets\img\Distributed.png" style="height: 100px;" />
        </div>
        <div class="col-sm-8">
            
            <b>Distributed Hyperdimensional Computing for Real-Time Data Aggregation and Interpretable Quality Monitoring in Manufacturing</b>
            <br />
            <i><a href="https://event.asme.org/IMECE" target="_blank" rel="noopener">IMECE 2024 (Porland, OR)</a></i> 
             <b style="color: #e74d3c">[Under Review]</b> 
            <br />
            <u>Zhiling Chen</u>, Danny Hoang, Ruimin Chen, Farhad Imani.
            <br />
             <a href="" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="" target="_blank" rel="noopener">[CODE]</a> 
             <!-- | <a href="https://youtu.be/-dho7l_r56U" target="_blank" rel="noopener">[DEMO]</a>  -->
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            The integration of diverse sensors in manufacturing processes offers enhanced potential for real-time quality assurance through the collection of complementary data. However, the limited interpretability of current machine learning models often hampers the effective discrimination of each sensor's unique contributions,
            <span class="collapse" id="more_scene_perception">
                primarily due to the complexity of decoding the interdependencies among various signals. This paper proposes a novel computational framework, Distributed Hyperdimensional Computing (DHDC), which is designed to leverage efficient cognitive operations, such as binding and bundling, for interpretable learning across multi-level sensor data. DHDC operates by encoding and aggregating data systematically across a distributed architecture, thereby enhancing transparency in computational efficiency. Our real-world experimental results on the 5-axis machining for the fabrication of the counterbore hole feature demonstrate that the framework not only effectively characterizes the impact of individual sensors but also achieves a high degree of predictive accuracy, as evidenced by an F1 score of $90.4 \pm 0.013\%$. The proposed framework holds the potential for interpretable and scalable quality control in distributed additive and subtractive manufacturing.
            </span> 
            <span> <a href="#more_scene_perception" data-toggle="collapse" onclick="toggleText(this)" id="link-more_scene_perception">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <!-- <div class="project" data-filter="perception manipulation">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_online.gif" />
        </div>
        <div class="col-sm-8">
            
            <b>Online 3D Deformable Object Classification for Mobile Cobot Manipulation</b>
            <br />
            <i><a href="https://www.isr-robotics.org/isr" target="_blank" rel="noopener">ISR Europe 2023 (Stuttgart, Baden-Wurttemberg, Germany)</a></i> 
            
            <br />
            <u>Khang Nguyen</u>, Tuan Dang, Manfred Huber.
            <br />
             <a href="https://www.researchgate.net/profile/Khang-Nguyen-133/publication/374371098_Online_3D_Deformable_Object_Classification_for_Mobile_Cobot_Manipulation/links/651a2cfa1e2386049def3947/Online-3D-Deformable-Object-Classification-for-Mobile-Cobot-Manipulation.pdf" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/mkhangg/deformable_cobot" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/qkgi3T6xYzI" target="_blank" rel="noopener">[DEMO]</a> 
             | <a href="https://mkhangg.com/assets/slides/isr23_slides.pdf" target="_blank" rel="noopener">[SLIDES]</a> 
             | <a href="https://youtu.be/ATzyXtLAK6E" target="_blank" rel="noopener">[TALK]</a> 
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            Vision-based object manipulation in assistive mobile cobots essentially relies on classifying the target objects based on their 3D shapes and features, whether they are deformed or not. In this work, we present an auto-generated dataset of deformed objects specific for assistive mobile cobot manipulation using an intuitive Laplacian-based mesh deformation procedure. We
            <span class="collapse" id="more_online">
                first determine the graspable region of the robot hand on the given object's mesh. Then, we uniformly sample handle points within the graspable region and perform deformation with multiple handle points based on the robot gripper configuration. In each deformation, we identify the orientation of handle points and prevent self-intersection to guarantee the object's physical meaning when multiple handle points are simultaneously applied to the mesh at different deformation intensities. We also introduce a lightweight neural network for 3D deformable object classification. Finally, we test our generated dataset on the Baxter robot with two 7-DOF arms, an integrated RGB-D camera, and a 3D deformable object classifier. The result shows that the robot is able to classify real-world deformed objects from point clouds captured at multiple views by the RGB-D camera.
            </span> 
            <span> <a href="#more_online" data-toggle="collapse" onclick="toggleText(this)" id="link-more_online">... See More</a></span>
        </div>                       
    </div>
</div>

            


            
                <div class="project" data-filter="framework">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_extperfc.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>ExtPerFC</i>:</b> 
            <b>An Efficient 2D &amp; 3D Perception Software-Hardware Framework for Mobile Cobot</b>
            <br />
            <i><a href="" target="_blank" rel="noopener">arXiv (06/08/2023)</a></i> 
            
            <br />
            Tuan Dang, <u>Khang Nguyen</u>, Manfred Huber.
            <br />
             <a href="https://arxiv.org/pdf/2306.04853.pdf" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/perception_framework" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/q4oz9Rixbzs" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            As the reliability of the robot's perception correlates with the number of integrated sensing modalities to tackle uncertainty, a practical solution to manage these sensors from different computers, operate them simultaneously, and maintain their real-time performance on the existing robotic system with minimal effort is needed. In this work, we present an end-to-end software-hardware
            <span class="collapse" id="more_extperfc">
                framework, namely <i>ExtPerFC</i>, that supports both conventional hardware and software components and integrates machine learning object detectors without requiring an additional dedicated graphic processor unit (GPU). We first design our framework to achieve real-time performance on the existing robotic system, guarantee configuration optimization, and concentrate on code reusability. We then mathematically model and utilize our transfer learning strategies for 2D object detection and fuse them into depth images for 3D depth estimation. Lastly, we systematically test the proposed framework on the Baxter robot with two 7-DOF arms, a four-wheel mobility base, and an Intel RealSense D435i RGB-D camera. The results show that the robot achieves real-time performance while executing other tasks (<i>e.g.</i>, map building, localization, navigation, object detection, arm moving, and grasping) simultaneously with available hardware like Intel onboard CPUs/GPUs on distributed computers. Also, to comprehensively control, program, and monitor the robot system, we design and introduce an end-user application.
            </span> 
            <span> <a href="#more_extperfc" data-toggle="collapse" onclick="toggleText(this)" id="link-more_extperfc">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="framework">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_perfc.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>PerFC</i>:</b> 
            <b>An Efficient 2D and 3D Perception Software-Hardware Framework for Mobile Cobot</b>
            <br />
            <i><a href="https://www.flairs-36.info/home" target="_blank" rel="noopener">FLAIRS-36 (Clearwater Beach, FL, U.S.)</a></i> 
            
            <br />
            Tuan Dang, <u>Khang Nguyen</u>, Manfred Huber.
            <br />
             <a href="https://journals.flvc.org/FLAIRS/article/view/133316/137627" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/perception_framework" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/q4oz9Rixbzs" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            In this work, we present an end-to-end software-hardware framework that supports both conventional hardware and software components and integrates machine learning object detectors without requiring an additional dedicated graphic processor unit (GPU). We design our framework to achieve real-time performance on the robot system, guarantee such performance on
            <span class="collapse" id="more_perfc">
                multiple computing devices, and concentrate on code reusability. We then utilize transfer learning strategies for 2D object detection and fuse them into depth images for 3D depth estimation. Lastly, we test the proposed framework on the Baxter robot with two 7-DOF arms and a four-wheel mobility base. The results show that the robot achieves real-time performance while executing other tasks (map building, localization, navigation, object detection, arm moving, and grasping) with available hardware like Intel onboard GPUs on distributed computers. Also, to comprehensively control, program, and monitor the robot system, we  design and introduce an end-user application.
            </span> 
            <span> <a href="#more_perfc" data-toggle="collapse" onclick="toggleText(this)" id="link-more_perfc">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_iotree.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>IoTree</i>:</b> 
            <b>A Battery-free Wearable System with Biocompatible Sensors for Continuous Tree Health Monitoring</b>
            <br />
            <i><a href="https://www.sigmobile.org/mobicom/2022/" target="_blank" rel="noopener">MobiCom 2022 (Sydney, NSW, Australia)</a></i> 
            
            <br />
            Tuan Dang, Trung Tran, <u>Khang Nguyen</u>, Tien Pham, Nhat Pham, Tam Vu, Phuc Nguyen.
            <br />
             <a href="https://dl.acm.org/doi/pdf/10.1145/3495243.3567652" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/iotree" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/8DUfOcuPwIk" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            In this paper, we present a low-maintenance, wind-powered, batteryfree, biocompatible, tree wearable, and intelligent sensing system, namely <i>IoTree</i>, to monitor water and nutrient levels inside a living tree. <i>IoTree</i> system includes tiny-size, biocompatible, and implantable sensors that
            <span class="collapse" id="more_iotree">
                continuously measure the impedance variations inside the living tree‚Äôs xylem, where water and nutrients are transported from the root to the upper parts. The collected data are then compressed and transmitted to a base station located at up to 1.8 kilometers (approximately 1.1 miles) away. The entire <i>IoTree</i> system is powered by wind energy and controlled by an adaptive computing technique called block-based intermittent computing, ensuring the forward progress and data consistency under intermittent power and allowing the firmware to execute with the most optimal memory and energy usage. We prototype <i>IoTree</i> that opportunistically performs sensing, data compression, and long-range communication tasks without batteries. During in-lab experiments, <i>IoTree</i> also obtains the accuracy of 91.08% and 90.51% in measuring 10 levels of nutrients, NH<sub>3</sub> and K<sub>2</sub>O, respectively. While tested with Burkwood Viburnum and White Bird trees in the indoor environment, <i>IoTree</i> data strongly correlated with multiple watering and fertilizing events. We also deployed <i>IoTree</i> on a grapevine farm for 30 days, and the system is able to provide sufficient measurements every day.
            </span> 
            <span> <a href="#more_iotree" data-toggle="collapse" onclick="toggleText(this)" id="link-more_iotree">... See More</a></span>
        </div>                       
    </div>
</div> -->

            
        </div>
        <p></p>
    </div>
</div>

<!-- Outreach section -->

<hr />

<div class="row" id="outreach">
    <div class="col">
        <h2 clss="mb-5">üìç outreach activities</h2>
        <p></p>
        
            <div class="row mb-4">
    <!-- <div class="col-sm-4">
        <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_spidey.gif" />
    </div> -->
    <!-- <div class="col-sm-8">
         <b><i>Spidey</i>:</b> 
        <b>An Autonomous Spatial Voice Localization Crawling Robot</b>
        <br />
        <i><a href="https://hackmit.org/" target="_blank" rel="noopener">HackMIT 2022 (Boston, MA, U.S.)</a></i>
        <br />
        <u>Khang Nguyen</u>.
        <br />
         <a href="https://spectacle.hackmit.org/project/185" target="_blank" rel="noopener">[POST]</a> 
         | <a href="https://github.com/mkhangg/hackmit22" target="_blank" rel="noopener">[CODE]</a> 
         | <a href="https://youtu.be/m1g6fkH6Zvg" target="_blank" rel="noopener">[DEMO]</a> 
        <br />
        <u><b><i>Description</i></b>:</u> We present an autonomous spatial voice localization crawling robot demonstrating the potential of assistive technology for people with visual impairment to ask for help whenever they are in a spatial area without physical assistance.
        <br />
        <u><b><i>Prize</i></b>:</u> Won Sponsorship Award for Assistive Technologies over 198 competing teams.
    </div> -->
</div>

<!--         
            <div class="row mb-4">
    <div class="col-sm-4">
        <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_iplanter.gif" />
    </div>
    <div class="col-sm-8">
         <b><i>iPlanter</i>:</b> 
        <b>An Autonomous Ground Monitoring and Tree Planting Robot</b>
        <br />
        <i><a href="https://robotech2022.devpost.com/" target="_blank" rel="noopener">GT IEEE RoboTech 2022 (Atlanta, GA, U.S.)</a></i>
        <br />
        <u>Khang Nguyen</u>, Muhtasim Mahfuz, Vincent Kipchoge, Johnwon Hyeon.
        <br />
         <a href="https://devpost.com/software/tree-planting-robot" target="_blank" rel="noopener">[POST]</a> 
         | <a href="https://github.com/mkhangg/robotech22" target="_blank" rel="noopener">[CODE]</a> 
         | <a href="https://youtu.be/GZ0oAX-lLSM" target="_blank" rel="noopener">[DEMO]</a> 
        <br />
        <u><b><i>Description</i></b>:</u> Our tree planting robot demonstrates the an on-farm surveying robot that autonomously determines soil quality, plants seeds, and collects on-ground images.
        <br />
        <u><b><i>Prize</i></b>:</u> Won 2nd place in Body Track, 3rd place in Electrical Track, and Top 8 Prizes over 47 competing teams (approximately 160 participants).
    </div>
</div> -->

        
        <p></p>
    </div>
</div>

<!-- Gallery section -->

<hr />

<div class="row" id="gallery">
    <div class="col">
        <h2 clss="mb-5">üìç gallery</h2>
        <p></p>
        not all about research, but also some happy moments of my life.
        <p></p>
        
            <div class="gallery">
    <img src="assets\memo\tulum.jpg" alt="Holiday at Tulum, Mexico" width="800" height="600" />
    <div class="desc">Tulum, Mexico</div>
</div>

        
            <!-- <div class="gallery">
    <img src="assets/memo/snow_football.jpg" alt="first time playing football on snow" width="800" height="600" />
    <div class="desc">first time playing football on snow (2022)</div>
</div>
   

        
            <div class="gallery">
    <img src="assets/memo/doituyentoan.jpg" alt="my boys from tƒën maths team" width="800" height="600" />
    <div class="desc">my boys from tƒën maths team (2017)</div>
</div> -->

        
        <p></p>
    </div>
</div>

<!-- Footer section -->

<div>‚Äé</div>
<hr />

<footer class="pt-2 my-md-2 pt-md-">
    <div class="row justify-content-center">
        <div class="col-7 col-md text-left align-self-center">
            <p class="h6">¬© Zhiling Chen, <span id="currentYear"></span></p>
            <!-- <a href="https://github.com/mkhangg/academic-website" target="_blank" rel="noopener"><b>&gt; web source @github</b></a> -->
        </div>
        <div class="col col-md text-right">
            
                <img class="mr+4" src="assets\img\uconn_engineering.jpg" data-canonical-src="assets\img\uconn_engineering.jpg" alt="UCONN Engineering" width="60" />
            
                <img class="mr+4" src="assets\img\uconn_logo.png" data-canonical-src="assets\img\uconn_logo.png" alt="UCONN" width="60" />
            
        </div>
    </div>
    <p></p>
</footer>


        </div>

        <!-- Bootstrap core JS-->
        <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>

        <!-- Isotope JS -->
        <script src="https://cdn.jsdelivr.net/npm/isotope-layout@3.0.2/dist/isotope.pkgd.min.js"></script>

        <!-- OwlCarousel2 JS -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js"></script>
        
		<!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>

		<!-- Core theme JS-->
        <script src="js/scripts.js"></script>

    </body>
</html>
