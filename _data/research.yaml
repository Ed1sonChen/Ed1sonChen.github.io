categories:

  - data-filter: HDC
    category-name: HDC

  - data-filter: VLMs
    category-name: VLMs

  - data-filter: RAG
    category-name: RAG

projects:

  - title: Distributed Hyperdimensional Computing for Real-Time Data Aggregation and Interpretable Quality Monitoring in Manufacturing
    system-name: 
    gif: assets\img\Distributed.png
    conference: IMECE 2024 (Porland, OR)
    conference-web: https://event.asme.org/IMECE
    status: Under Review
    authors: <u>Zhiling Chen</u>, Danny Hoang, Ruimin Chen, Farhad Imani.
    pdf: 
    # code: https://github.com/mkhangg/semantic_scene_perception
    # demo: https://youtu.be/-dho7l_r56U
    slides: 
    talk: 
    abstract-less: The integration of diverse sensors in manufacturing processes offers enhanced potential for real-time quality assurance through the collection of complementary data. However, the limited interpretability of current machine learning models often hampers the effective discrimination of each sensor's unique contributions,
    abstract-more: primarily due to the complexity of decoding the interdependencies among various signals. This paper proposes a novel computational framework, Distributed Hyperdimensional Computing (DHDC), which is designed to leverage efficient cognitive operations, such as binding and bundling, for interpretable learning across multi-level sensor data. DHDC operates by encoding and aggregating data systematically across a distributed architecture, thereby enhancing transparency in computational efficiency. Our real-world experimental results on the 5-axis machining for the fabrication of the counterbore hole feature demonstrate that the framework not only effectively characterizes the impact of individual sensors but also achieves a high degree of predictive accuracy, as evidenced by an F1 score of $90.4 \pm 0.013\%$. The proposed framework holds the potential for interpretable and scalable quality control in distributed additive and subtractive manufacturing.
    tag: more_scene_perception
    category: HDC

  - title: Vision Language Model for Interpretable and Fine-grained Detection of Safety Compliance in Diverse Workplaces
    system-name: 
    gif: assets/img/vlm_ppe.png
    # Arxiv: ISR Europe 2023 (Stuttgart, Baden-Wurttemberg, Germany)
    # Link: https://www.isr-robotics.org/isr
    status: Under Review
    authors: <u>Zhiling Chen</u>, Hanning Chen, Mohsen Imani, Ruimin Chen, Farhad Imani.
    pdf: https://arxiv.org/pdf/2408.07146
    # code: https://github.com/mkhangg/deformable_cobot
    # demo: https://youtu.be/qkgi3T6xYzI
    # slides: https://mkhangg.com/assets/slides/isr23_slides.pdf
    # talk: https://youtu.be/ATzyXtLAK6E
    abstract-less: Workplace accidents due to personal protective equipment (PPE) non-compliance raise serious safety concerns and lead to legal liabilities, financial penalties, and reputational damage. While object detection models have shown the capability to address this issue by identifying safety items, most existing models, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the fine-grained attributes of PPE across diverse workplace scenarios. Vision language models (VLMs) are gaining traction for detection tasks by leveraging the synergy between visual and textual information, offering a promising solution to traditional object detection limitations in PPE recognition. Nonetheless,
    abstract-more: VLMs face challenges in consistently verifying PPE attributes due to the complexity and variability of workplace environments, requiring them to interpret context-specific language and visual cues simultaneously. We introduce Clip2Safety, an interpretable detection framework for diverse workplace safety compliance, which comprises four main modules:scene recognition, the visual prompt, safety items detection, and fine-grained verification. The scene recognition identifies the current scenario to determine the necessary safety gear. The visual prompt formulates the specific visual prompts needed for the detection process. The safety items detection identifies whether the required safety gear is being worn according to the specified scenario. Lastly, the fine-grained verification assesses whether the worn safety equipment meets the fine-grained attribute requirements. We conduct real-world case studies across six different scenarios. The results show that Clip2Safety not only demonstrates an accuracy improvement over state-of-the-art question-answering based VLMs but also achieves inference times two hundred times faster.
    category: VLMs

